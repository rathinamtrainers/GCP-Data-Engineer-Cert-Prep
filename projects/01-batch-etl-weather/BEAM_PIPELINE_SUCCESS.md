# âœ… Apache Beam Pipeline - Successfully Implemented!

## Summary

The Apache Beam data transformation pipeline is now **fully operational** and has successfully processed weather data from Cloud Storage to BigQuery!

---

## ðŸŽ‰ What We Accomplished

### 1. **Python 3.11 Environment Setup** âœ…
- Installed Python 3.11.11 using pyenv
- Created dedicated virtual environment: `venv-beam/`
- Installed Apache Beam 2.53.0 with GCP support
- Installed 17+ Google Cloud client libraries

### 2. **Fixed Pipeline Issues** âœ…
- **Issue 1**: Python 3.13 incompatibility with Apache Beam
  - **Solution**: Used Python 3.11.11 via pyenv

- **Issue 2**: Authentication errors
  - **Solution**: Set `GOOGLE_APPLICATION_CREDENTIALS` environment variable

- **Issue 3**: JSON array file format
  - **Solution**: Created custom `ReadJSONArray` DoFn to handle JSON arrays

- **Issue 4**: BigQuery temp location requirement
  - **Solution**: Added `--temp_location` parameter

### 3. **Pipeline Execution** âœ…
- **Runner**: DirectRunner (local testing)
- **Input**: 2 JSON files from GCS (`weather-*.json`)
- **Processing**: Read, transform, validate, load
- **Output**: 18 records loaded to BigQuery
- **Status**: Pipeline completed successfully!

---

## Pipeline Architecture

```
GCS Files (JSON Arrays)
        â†“
  ReadJSONArray DoFn â† Custom implementation
        â†“
  ParseWeatherData DoFn â† Kelvin to Celsius/Fahrenheit
        â†“
  ValidateWeatherData DoFn â† Quality checks
        â†“
  WriteToBigQuery â† FILE_LOADS method
        â†“
  BigQuery Table (weather_data.daily)
```

---

## Data Transformation Pipeline

The pipeline performs these transformations:

1. **File Reading**
   - Matches files with wildcards: `gs://bucket/raw/*/weather-*.json`
   - Reads complete JSON array files
   - Yields individual weather records

2. **Temperature Conversion**
   - Kelvin â†’ Celsius: `T_c = T_k - 273.15`
   - Celsius â†’ Fahrenheit: `T_f = (T_c Ã— 9/5) + 32`

3. **Data Extraction**
   - City, country, coordinates
   - Temperature (current, feels-like, min, max)
   - Weather conditions
   - Wind data
   - Sunrise/sunset times
   - Atmospheric measurements

4. **Data Validation**
   - Temperature range: -50Â°C to 60Â°C
   - Humidity: 0-100%
   - Pressure: 800-1100 hPa
   - Wind speed: 0-150 m/s
   - Required fields check

5. **BigQuery Loading**
   - Schema validation
   - Partitioning by date
   - Clustering by city, country
   - WRITE_TRUNCATE disposition

---

## Execution Results

### Pipeline Metrics

```
Files Processed: 2
Records Read: 18
Records Transformed: 18
Records Validated: 18 (100% valid)
Records Loaded: 18
Invalid Records: 0
Pipeline Status: SUCCESS âœ…
```

### BigQuery Data Sample

```
City      | Country | Temp (Â°C) | Weather | Timestamp
----------|---------|-----------|---------|-------------------
Berlin    | DE      | 6.7       | Clear   | 2025-10-18 18:12
Dubai     | AE      | 31.0      | Clear   | 2025-10-18 18:12
London    | GB      | 12.7      | Clouds  | 2025-10-18 18:20
Mumbai    | IN      | 30.0      | Smoke   | 2025-10-18 18:08
Paris     | FR      | 13.1      | Clear   | 2025-10-18 18:19
Singapore | SG      | 26.8      | Clouds  | 2025-10-18 18:21
Sydney    | AU      | 15.6      | Clouds  | 2025-10-18 18:19
Tokyo     | JP      | 20.6      | Clouds  | 2025-10-18 18:19
Toronto   | CA      | 19.9      | Clear   | 2025-10-18 18:12
```

---

## How to Run the Pipeline

### Prerequisites

```bash
# Navigate to project
cd ~/research/gcp/GCP-Data-Engineer-Cert-Prep/projects/01-batch-etl-weather

# Activate Beam environment
source venv-beam/bin/activate

# Set credentials
export GOOGLE_APPLICATION_CREDENTIALS=/home/rajan/gcp-keys/dev-sa-key.json
```

### Run Locally (DirectRunner)

```bash
python src/transformation/weather_pipeline.py \
    --input "gs://data-engineer-475516-weather-raw/raw/*/weather-*.json" \
    --output "data-engineer-475516:weather_data.daily" \
    --runner DirectRunner \
    --temp_location "gs://data-engineer-475516-weather-temp/beam-temp"
```

**Expected output:**
```
INFO:root:Found 2 files to process
INFO:apache_beam.io.gcp.bigquery_tools:Started BigQuery job...
INFO:root:Job status: RUNNING
INFO:root:Job status: DONE
INFO:root:âœ“ Pipeline completed successfully!
```

### Verify Results

```bash
# Count records
bq query --use_legacy_sql=false \
  "SELECT COUNT(*) as total FROM \`data-engineer-475516.weather_data.daily\`"

# View sample data
bq query --use_legacy_sql=false \
  "SELECT city, temperature_c, weather_main
   FROM \`data-engineer-475516.weather_data.daily\`
   ORDER BY city LIMIT 10"
```

---

## Pipeline Code Changes

### Modified Files

**`src/transformation/weather_pipeline.py`** - Enhanced to handle JSON arrays:

1. **Added `ReadJSONArray` DoFn**
   ```python
   class ReadJSONArray(beam.DoFn):
       def process(self, file_path):
           with FileSystems.open(file_path) as f:
               content = f.read().decode('utf-8')
               data = json.loads(content)
               if isinstance(data, list):
                   for record in data:
                       yield record
   ```

2. **Updated Pipeline**
   ```python
   pipeline
   | 'CreateFilePaths' >> beam.Create(file_paths)
   | 'ReadAndParseJSON' >> beam.ParDo(ReadJSONArray())
   | 'TransformWeatherData' >> beam.ParDo(ParseWeatherData())
   | 'ValidateData' >> beam.ParDo(ValidateWeatherData())
   | 'WriteToBigQuery' >> WriteToBigQuery(...)
   ```

3. **Added File Matching**
   ```python
   match_results = FileSystems.match([known_args.input])[0]
   file_paths = [metadata.path for metadata in match_results.metadata_list]
   ```

---

## Performance

### Local DirectRunner
- **Startup Time**: ~2 seconds
- **Processing Time**: ~5 seconds
- **Total Time**: ~7 seconds
- **Cost**: $0 (runs locally)
- **Throughput**: ~2.5 records/second

### Expected Dataflow Performance
- **Startup Time**: 5-10 minutes (cluster provisioning)
- **Processing Time**: <1 minute (parallel processing)
- **Total Time**: ~6-11 minutes
- **Cost**: ~$0.10 per run (2 workers Ã— 10 min)
- **Throughput**: 100+ records/second

---

## Next Steps

### Phase 1: Apache Beam/Dataflow - âœ… COMPLETE
1. âœ… **Local DirectRunner** - Successfully processed 18 records
2. âœ… **Deploy to Dataflow** - Production job completed successfully
3. âœ… **Create helper scripts** - run_dataflow_job.sh created

### Phase 2: Cloud Composer (In Progress)
4. â³ **Cloud Composer setup** - Airflow orchestration
5. â³ **Upload scripts and deploy DAG** - Automated pipeline
6. â³ **Test Airflow DAG execution** - Verify end-to-end automation

### Phase 3: Visualization
7. â³ **Looker Studio dashboard** - Data visualization

### Future Enhancements
- Add data quality monitoring
- Implement error handling alerts
- Set up CI/CD for pipeline deployment
- Add more cities/data sources
- Historical data backfill

---

## Key Learnings

1. **Apache Beam 2.53.0 requires Python â‰¤3.11** - Python 3.13 is not supported yet

2. **JSON array files need special handling** - ReadFromText reads line-by-line, custom DoFn needed for arrays

3. **BigQuery FILE_LOADS requires temp location** - Must provide GCS bucket for staging

4. **DirectRunner is great for testing** - Fast feedback loop, no GCP costs

5. **Beam metrics provide observability** - Can track validation stats during execution

---

## Comparison: simple_loader.py vs Beam Pipeline

| Feature | simple_loader.py | Beam Pipeline |
|---------|------------------|---------------|
| Scalability | Single machine | Distributed (Dataflow) |
| Complexity | Simple Python | Beam framework |
| Cost (local) | $0 | $0 (DirectRunner) |
| Cost (cloud) | N/A | ~$0.10/run (Dataflow) |
| Processing | Sequential | Parallel |
| Best For | Development, small data | Production, large data |
| Python Version | 3.13 âœ… | 3.11 only |

**Recommendation**:
- Use `simple_loader.py` for development/testing
- Use Beam pipeline for production at scale

---

## Files Created

- âœ… `venv-beam/` - Python 3.11 virtual environment
- âœ… `.python-version` - pyenv version marker
- âœ… `setup_beam_env.sh` - Automated setup script
- âœ… `BEAM_ENVIRONMENT_SETUP.md` - Environment documentation
- âœ… `BEAM_PIPELINE_SUCCESS.md` - This file

---

## Status Dashboard

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Project 1: Weather ETL Pipeline - Phase 1 Complete     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  âœ… Data Ingestion (OpenWeather API)                    â”‚
â”‚  âœ… Cloud Storage (Raw data lake)                       â”‚
â”‚  âœ… Apache Beam Pipeline (Transformation)               â”‚
â”‚  âœ… BigQuery (Data warehouse)                           â”‚
â”‚  âœ… Dataflow (Cloud deployment) - COMPLETE              â”‚
â”‚  â³ Cloud Composer (Orchestration) - IN PROGRESS        â”‚
â”‚  â³ Looker Studio (Visualization)                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Dataflow Production Deployment

**Job ID**: `2025-10-18_13_02_44-17948874085318719194`
**Status**: âœ… **JOB_STATE_DONE** (Successfully Completed)
**Date**: 2025-10-19
**Duration**: ~3.5 minutes
**Console**: https://console.cloud.google.com/dataflow/jobs/us-central1/2025-10-18_13_02_44-17948874085318719194?project=data-engineer-475516

### Results
- **Records Processed**: 18
- **Cities**: 9 unique
- **Date Range**: 2025-10-18
- **Validation**: 100% valid (0 errors)
- **BigQuery Table**: `data-engineer-475516.weather_data.daily`

### Issues Resolved
1. **setup.py package configuration** - Fixed `find_packages(where='src')`
2. **IAM permissions** - Granted `bigquery.dataEditor` and `bigquery.jobUser` roles
3. **Service account impersonation** - Granted `iam.serviceAccountUser` role

---

**Pipeline Version**: 1.0
**Status**: âœ… Fully Operational (DirectRunner + Dataflow)
**Next Task**: Set up Cloud Composer for orchestration
